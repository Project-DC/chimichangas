{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # set random seed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import world\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear all previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists('Players_Data'):\n",
    "    shutil.rmtree('Players_Data/')\n",
    "    os.mkdir('Players_Data')\n",
    "else:\n",
    "    os.mkdir('Players_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial states of the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 21\n",
    "ACTION_SIZE = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_state(state, maxlen):\n",
    "    if len(state) > maxlen:\n",
    "        return state[:maxlen]\n",
    "    elif len(state) < maxlen:\n",
    "        new_state = np.zeros((maxlen,))\n",
    "        new_state[:len(state)] = state\n",
    "        return new_state\n",
    "    elif len(state) == maxlen:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_single(players, my_particles, killed, i):\n",
    "    global STATE_SIZE\n",
    "    \n",
    "    initial_state = []\n",
    "    if type(players[i]) != int:\n",
    "        env_particles,env_particle_distance = food_in_env(players[i], my_particles)\n",
    "        env_food_vector = getFoodVector(players[i],env_particles, my_particles)\n",
    "        env_food_vector = sum(env_food_vector, [])\n",
    "\n",
    "        env_players, env_player_distance = players_in_env(players[i],players)\n",
    "        env_player_vector = getPlayerVector(players[i],env_players, players)\n",
    "        env_player_vector = sum(env_player_vector, [])\n",
    "\n",
    "        temp_state = [env_food_vector, env_player_vector]\n",
    "        temp_state = sum(temp_state, [])\n",
    "        initial_state.append(np.array(temp_state))\n",
    "    else:\n",
    "        initial_state.append(np.array([0]))\n",
    "\n",
    "    initial_state = [np.append(initial_state[i], players[i].energy) if type(players[i]) != int else np.append(initial_state[i], -100) for i in range(1)]\n",
    "\n",
    "    return np.array(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(players, my_particles, killed):\n",
    "    global STATE_SIZE\n",
    "    \n",
    "    initial_state = []\n",
    "    for i in range(len(players)):\n",
    "        if type(players[i]) != int:\n",
    "            env_particles,env_particle_distance = food_in_env(players[i], my_particles)\n",
    "            env_food_vector = getFoodVector(players[i],env_particles, my_particles)\n",
    "            env_food_vector = sum(env_food_vector, [])\n",
    "\n",
    "            env_players, env_player_distance = players_in_env(players[i],players)\n",
    "            env_player_vector = getPlayerVector(players[i],env_players, players)\n",
    "            env_player_vector = sum(env_player_vector, [])\n",
    "\n",
    "            temp_state = [env_food_vector, env_player_vector]\n",
    "            temp_state = sum(temp_state, [])\n",
    "            initial_state.append(np.array(temp_state))\n",
    "        else:\n",
    "            initial_state.append(np.array([0]))\n",
    "\n",
    "    initial_state = [pad_state(state, STATE_SIZE-1) for state in initial_state]\n",
    "    initial_state = [np.append(initial_state[i], players[i].energy) if type(players[i]) != int else np.append(initial_state[i], -100) for i in range(len(players))]\n",
    "\n",
    "    return np.array(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Architecture of the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, s_size=STATE_SIZE, h_size=30, a_size=ACTION_SIZE):\n",
    "        super(Agent, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent with REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    agents = [Agent().to(device) for _ in range(world.INITIAL_POPULATION)]\n",
    "    optimizers = [optim.Adam(agent.parameters(), lr=1e-2) for agent in agents]\n",
    "    \n",
    "    TIME = -1\n",
    "    regenerate_times = 0\n",
    "    MAX_REGENERATIONS = 100\n",
    "    allow_regenerate = True\n",
    "    FOOD_REGEN_CONDITION_IS_MET = False\n",
    "\n",
    "    \n",
    "    players, killed, my_particles = world.init()\n",
    "    \n",
    "    states = get_state(players, my_particles, killed)\n",
    "\n",
    "    scores = [0 for _ in range(len(players))]\n",
    "    saved_log_probs = {i:[] for i in range(len(players))}\n",
    "    rewards = {i:[] for i in range(len(players))}\n",
    "    \n",
    "    while True:\n",
    "        if(len(killed) == len(players)):\n",
    "            print(killed)\n",
    "            break\n",
    "        TIME += 1\n",
    "        for i, agent in enumerate(agents):\n",
    "            if type(players[i]) != int:\n",
    "                action, log_prob = agents[i].act(states[i])\n",
    "                saved_log_probs[i].append(log_prob)\n",
    "                reward, done, players, my_particles, killed, mate_idx, TIME = world.take_action(players, my_particles, killed, i, action, TIME)\n",
    "                rewards[i].append(reward)\n",
    "                \n",
    "                if(action == 10 and reward == 0):\n",
    "                    print(\"Asexual reproduction\")\n",
    "                    offsprings = len(players) - len(agents)\n",
    "                    for j in range(len(agents), len(agents) + offsprings):\n",
    "                        agents.append(Agent().to(device))\n",
    "                        agents[-1].load_state_dict(agents[i].state_dict())\n",
    "                        optimizers.append(optim.Adam(agents[-1].parameters(), lr=1e-2))\n",
    "                        states[i] = 0\n",
    "                        scores.append(0)\n",
    "                        saved_log_probs[j] = []\n",
    "                        rewards[j] = []\n",
    "                elif(action == 11 and reward == 4):\n",
    "                    print(\"Sexual reproduction\")\n",
    "                    dominant_percent = random.randint(0, 10) * 10\n",
    "                    recessive_percent = 100 - dominant_percent\n",
    "                    offsprings = len(players) - len(agents)\n",
    "                    num_dominant = round(offsprings * (dominant_percent / 100))\n",
    "                    num_recessive = offsprings - num_dominant\n",
    "                    \n",
    "                    for j in range(len(agents), len(agents) + num_dominant):\n",
    "                        agents.append(Agent().to(device))\n",
    "                        agents[-1].load_state_dict(agents[i].state_dict())\n",
    "                        optimizers.append(optim.Adam(agents[-1].parameters(), lr=1e-2))\n",
    "                        scores.append(0)\n",
    "                        saved_log_probs[j] = []\n",
    "                        rewards[j] = []\n",
    "                    for j in range(len(agents) + num_dominant, len(agents) + num_dominant + num_recessive):\n",
    "                        agents.append(Agent().to(device))\n",
    "                        agents[-1].load_state_dict(agents[mate_idx].state_dict())\n",
    "                        optimizers.append(optim.Adam(agents[-1].parameters(), lr=1e-2))\n",
    "                        scores.append(0)\n",
    "                        saved_log_probs[j] = []\n",
    "                        rewards[j] = []\n",
    "                \n",
    "                if(type(players[i]) == int):\n",
    "                    agents[i] = 0\n",
    "                \n",
    "                if(TIME % 10):\n",
    "                    optimizers[i].zero_grad()\n",
    "                    policy_loss = 0\n",
    "                    for j in range(len(saved_log_probs[i])):\n",
    "                        policy_loss += (-saved_log_probs[i][j] * rewards[i][j])\n",
    "                    policy_loss.backward(retain_graph=True)\n",
    "                    optimizers[i].step()\n",
    "\n",
    "                next_states = get_state(players, my_particles, killed)\n",
    "                rewards[i].append(reward)\n",
    "                scores[i] += reward\n",
    "                states = next_states\n",
    "            \n",
    "#         if(len(killed) == len(players) and allow_regenerate):\n",
    "#             discounts = {j:[gamma**i for i in range(len(rewards[j])+1)] for j in range(len(players))}\n",
    "#             R = {j:sum([a*b for a,b in zip(discounts[j], rewards[j])]) for j in range(len(players))}\n",
    "            \n",
    "#             policy_loss = {i:[] for i in range(len(players))}\n",
    "#             for i, saved_log_prob in saved_log_probs.items():\n",
    "#                 for log_prob in saved_log_prob:\n",
    "#                     policy_loss[i].append(-log_prob * R[i])\n",
    "#                 policy_loss[i] = torch.cat(policy_loss[i]).sum()\n",
    "            \n",
    "#                 optimizers[i].zero_grad()\n",
    "#                 policy_loss[i].backward(retain_graph=True)\n",
    "#                 optimizers[i].step()\n",
    "            \n",
    "#             killed = []\n",
    "#             players = regenerate_species(TIME)\n",
    "#             print(\"GENERATION:\", regenerate_times, \", score:\", scores)\n",
    "#             regenerate_times += 1\n",
    "#         elif(len(killed) == INITIAL_POPULATION and not allow_regenerate):\n",
    "#             running = False\n",
    "\n",
    "#         if(regenerate_times == MAX_REGENERATIONS):\n",
    "#             allow_regenerate = False\n",
    "#             break\n",
    "        \n",
    "        \n",
    "#         if i_episode % print_every == 0:\n",
    "#             print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "#         if np.mean(scores_deque)>=200.0:\n",
    "#             print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "#             break\n",
    "        \n",
    "#     return scores\n",
    "    \n",
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Plot the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
